"""Functions for systematically evaluating a function over specific cases.
"""
import itertools

import numpy as np
import xarray as xr
from dask.delayed import delayed, compute

from ..parallel import DaskTqdmProgbar, _dask_get
from ..utils import _parse_fn_name, progbar


def case_runner(fn, fn_args, cases,
                constants=None,
                split=False,
                parallel=False,
                num_workers=None,
                parallel_backend='MULTIPROCESSING',
                hide_progbar=False,
                progbar_opts=None):
    """ Evaluate a function in many different configurations, optionally in
    parallel and or with live progress.

    Parameters
    ----------
        fn:
            function with which to evalute cases with

        fn_args:
            names of case arguments that fn takes

        cases:
            list settings that fn_args take

        constants:
            constant fn args that won't be iterated over

        split:
            whether to split fn's output into multiple lists

        progbars:
            whether to show (in this case only 1) progbar

        parallel:
            whether to evaluate cases in parallel

        processes:
            how any processes to use for parallel processing

        progbar_opts:
            options to send to progbar


    Returns
    -------
        results: list of fn output for each case
    """

    # Prepare Function
    constants = dict() if constants is None else dict(constants)
    fn_name = _parse_fn_name(fn)

    # Prepate fn_args and values
    if isinstance(fn_args, str):
        fn_args = (fn_args,)
        cases = tuple((c,) for c in cases)

    if progbar_opts is None:
        progbar_opts = dict()

    # Evaluate configurations in parallel
    if parallel or num_workers:
        with DaskTqdmProgbar(fn_name, disable=hide_progbar, **progbar_opts):
            jobs = [delayed(fn)(**constants, **dict(zip(fn_args, case)))
                    for case in cases]
            getter = (_dask_get(parallel_backend, num_workers=num_workers) if
                      parallel_backend is not None else None)
            results = compute(*jobs, get=getter, num_workers=num_workers)

    # Evaluate configurations sequentially
    else:
        results = tuple(fn(**constants, **dict(zip(fn_args, case)))
                        for case in progbar(cases, total=len(cases),
                                            disable=hide_progbar))

    return tuple(zip(*results)) if split else results


def find_union_coords(cases):
    """Take a list of cases and find the union of coordinates
    with which to index all cases. Sort the coords if possible.
    """
    for x in zip(*cases):
        try:
            yield sorted(list(set(x)))
        except TypeError:  # unsortable
            yield list(set(x))


def all_missing_ds(coords, var_names, var_dims, var_types):
    """ Make a dataset whose data is all missing. """
    # Blank dataset with appropirate coordinates
    ds = xr.Dataset(coords=coords)
    for v_name, v_dims, v_type in zip(var_names, var_dims, var_types):
        shape = tuple(ds[d].size for d in v_dims)
        if v_type == int or v_type == float:
            # Warn about upcasting int to float?
            nodata = np.tile(np.nan, shape)
        elif v_type == complex:
            nodata = np.tile(np.nan + np.nan*1.0j, shape)
        else:
            nodata = np.tile(None, shape).astype(object)
        ds[v_name] = (v_dims, nodata)
    return ds


def cases_to_ds(results, fn_args, cases, var_names, var_dims=None,
                var_coords=None, add_to_ds=None, overwrite=False):
    """ Take a list of results and configurations that generate them and turn it
    into a `xarray.Dataset`.

    Parameters
    ----------
        results: list(s) of results of len(cases), e.g. generated by
            `case_runner`.
        fn_args: arguments used in function that generated the results
        cases: list of configurations used to generate results
        var_names: name(s) of output variables for a single result
        var_dims: the list of named coordinates for each single result
            variable, i.e. coordinates not generated by the combo_runner
        var_coords: dict of values for those coordinates if custom ones are
            desired.

    Returns
    -------
        ds: Dataset holding all results, with coordinates described by cases

    Notes
    -----
        1. Many data types have to be converted to object in order for the
            missing values to be represented by NaNs.
    """
    # Prepare fn_args/cases var_names/results
    if isinstance(fn_args, str):
        fn_args = (fn_args,)
        cases = tuple((c,) for c in cases)

    # Prepare var_names/dims/results
    if isinstance(var_names, str):
        var_names = (var_names,)
        results = tuple((r,) for r in results)
        if var_dims is not None:
            var_dims = (var_dims,)

    if var_coords is None:
        var_coords = dict()

    if add_to_ds is None:
        # Allow single given dimensions to represent all result variables
        var_dims = (itertools.cycle(var_dims) if var_dims is not None else
                    itertools.repeat(tuple()))

        # Find minimal covering set of coordinates for fn_args
        case_coords = dict(zip(fn_args, find_union_coords(cases)))

        # Create new, 'all missing' dataset if required
        ds = all_missing_ds(coords={**case_coords, **dict(var_coords)},
                            var_names=var_names,
                            var_dims=(tuple(fn_args) + tuple(next(var_dims))
                                      for i in range(len(var_names))),
                            var_types=(np.asarray(x).dtype
                                       for x in results[0]))
    else:
        ds = add_to_ds

    #  go through cases, overwriting nan with results
    for res, cfg in zip(results, cases):
        for vname, x in zip(var_names, res):
            if not overwrite:
                if not ds[vname].loc[dict(zip(fn_args, cfg))].isnull().all():
                    raise ValueError("Existing data and `overwrite` = False")
            try:
                len(x)
                ds[vname].loc[dict(zip(fn_args, cfg))] = np.asarray(x)
            except TypeError:
                ds[vname].loc[dict(zip(fn_args, cfg))] = x

    return ds


def case_runner_to_ds(fn, fn_args, cases, var_names,
                      var_dims=None,
                      var_coords=None,
                      add_to_ds=None,
                      overwrite=False,
                      **case_runner_settings):
    """ Combination of `case_runner` and `cases_to_ds`. Takes a function and
    list of argument configurations and produces a `xarray.Dataset`.

    Parameters
    ----------
        fn: function to evaluate
        fn_args: names of function args
        cases: list of function arg configurations
        var_names: list of names of single fn output
        var_dims: list of list of extra dims for each fn output
        var_coords: dictionary describing custom values of var_dims
        case_runner_settings: dict to supply to `case_runner`

    Returns
    -------
        ds: dataset with minimal covering coordinates and all cases
            evaluated.
    """
    if var_coords is None:
        var_coords = dict()

    # Generate results
    results = case_runner(fn, fn_args, cases, **case_runner_settings)
    # Convert to xarray.Dataset
    ds = cases_to_ds(results, fn_args, cases,
                     var_names=var_names,
                     var_dims=var_dims,
                     var_coords=var_coords,
                     add_to_ds=add_to_ds,
                     overwrite=overwrite)
    return ds


# --------------------------------------------------------------------------- #
# Update or add new values                                                    #
# --------------------------------------------------------------------------- #

def find_missing_cases(ds, var_dims=None):
    """ Find all cases in a dataset with missing data.

    Parameters
    ----------
        ds: Dataset in which to find missing data
        var_dims: internal variable dimensions (i.e. to ignore)

    Returns
    -------
        (m_fn_args, m_cases): function arguments and missing cases.
    """
    # Parse var_dims
    var_dims = (() if var_dims is None else
                (var_dims,) if isinstance(var_dims, str) else
                var_dims)
    # Find all configurations
    fn_args = tuple(coo for coo in ds.coords if coo not in var_dims)
    var_names = tuple(ds.data_vars)
    all_cases = itertools.product(*(ds[arg].data for arg in fn_args))

    # Only return those corresponding to all missing data
    def gen_missing_list():
        for case in all_cases:
            sub_ds = ds.loc[dict(zip(fn_args, case))]
            if all(sub_ds[v].isnull().all() for v in var_names):
                yield case

    return fn_args, tuple(gen_missing_list())


def fill_missing_cases(ds, fn, var_names, var_dims=None, var_coords=None,
                       **case_runner_settings):
    """ Take a dataset and function etc. and fill its missing data in

    Parameters
    ----------
        ds: Dataset to analyse and fill
        fn: function to use to fill missing cases
        var_names: output variable names of function
        var_dims: output varialbe named dimensions of function
        var_coords: dictionary of coords for output dims
        **case_runner_settings: settings sent to `case_runner`
    Returns
    -------
        None: (filling performed in-place)
    """
    if var_coords is None:
        var_coords = dict()

    # Find missing cases
    fn_args, missing_cases = find_missing_cases(ds, var_dims)
    # Evaluate and add to Dataset
    case_runner_to_ds(fn, fn_args, missing_cases,
                      var_names=var_names,
                      var_dims=var_dims,
                      var_coords=var_coords,
                      add_to_ds=ds,
                      **case_runner_settings)
